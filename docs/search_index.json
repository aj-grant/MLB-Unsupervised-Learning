[["index.html", "Machine Learning for Biostatistics Module 7 Unsupervised Learning Introduction Dataset used in the examples Slides from the videos", " Machine Learning for Biostatistics Module 7 Armando Teixeira-Pinto, Jaroslaw Harezlak &amp; Andrew Grant 2025-09-15 Unsupervised Learning Introduction Supervised learning, in machine learning, refers to methods that are applied when we want to estimate the function \\(f(X)\\) that relates a group of predictors \\(X\\) to a measured outcome \\(Y\\). Unsupervised learning refers to methods that learn from the data but there is no observed outcome. In this module, we will cover several unsupervised learning methods, namely principal components analysis, k-means clustering and hierarchical clustering. By the end of this module you should be able to: Implement dimension reduction using principal components analysis Implement clustering methods Dataset used in the examples The dataset fat is available in the library(faraway). You may need to install this library. The data set contains several physical measurements of 252 males. Most of the variables can be measured with a scale or tape measure. Can they be used to predict the percentage of body fat? If so, this offers an easy alternative to an underwater weighing technique. Data frame with 252 observations on the following 19 variables. The data were generously supplied by Dr. A. Garth Fisher, Human Performance Research Center, Brigham Young University, Provo, Utah 84602, who gave permission to freely distribute the data and use them for non-commercial purposes. Reference to the data is made in Penrose, et al. (1985). Variables: brozek – Percent body fat using Brozek’s equation, 457/Density - 414.2 siri – Percent body fat using Siri’s equation, 495/Density - 450 density – Density (gm/cm^2) age – Age (yrs) weight – Weight (lbs) height – Height (inches) adipos – BMI Adiposity index = Weight/Height^2 (kg/m^2) free – Fat Free Weight = (1 - fraction of body fat) * Weight, using Brozek’s formula (lbs) neck – Neck circumference (cm) chest – Chest circumference (cm) abdom – Abdomen circumference (cm) “at the umbilicus and level with the iliac crest” hip – Hip circumference (cm) dthigh – Thigh circumference (cm) knee – Knee circumference (cm) ankle – Ankle circumference (cm) biceps – Extended biceps circumference (cm) forearm – Forearm circumference (cm) wrist – Wrist circumference (cm) “distal to the styloid processes” The dataset bdiag.csv contains quantitative information from digitized images of a diagnostic test (fine needle aspirate (FNA) test on breast mass) for the diagnosis of breast cancer. The variables describe characteristics of the cell nuclei present in the image. Variables Information: ID number Diagnosis (M = malignant, B = benign) and ten real-valued features are computed for each cell nucleus: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (“coastline approximation” - 1) The mean, standard error and “worst” or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius. This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/ Slides from the videos You can download the slides used in the videos from Unsupervised Learning: Slides "],["principal-components-analysis.html", "1 Principal components analysis 1.1 Introduction 1.2 Readings 1.3 Practice session 1.4 Exercises", " 1 Principal components analysis 1.1 Introduction Principal components analysis (PCA) is a data reduction (also referred to as dimension reduction) technique. Given a large set of variables, PCA identifies a small number of linear combinations (components) that retain most of the information of the variables. Suppose we have \\(p\\) variables \\(x_1,...,x_p\\). A PCA analysis would identify, \\(z_1,z_2,...\\) components that are linear combinations of the original variables, \\[ \\begin{aligned} z_1 = \\phi_{11}x_1 + &amp;\\phi_{21}x_2 +... + \\phi_{p1}x_p\\\\ z_2 = \\phi_{12}x_1 + &amp;\\phi_{22}x_2 +... + \\phi_{p2}x_p\\\\ z_3 = \\phi_{13}x_1 + &amp;\\phi_{23}x_2 +... + \\phi_{p3}x_p\\\\ \\vdots \\end{aligned} \\] We will have up to \\(p\\) components (unless the number of observations is less than \\(p\\)), but hopefully the first components will explain most of the information and we can discard the others. This is why PCA is called a data reduction method. There are two extreme situations: all the variables were completely independent and in this case, the number of “important” components would be the same as the number of variables, or all the variable are perfectly correlated so 1 component would retain all the information. For simplicity, let’s assume that variables \\(x_1,...,x_p\\) are scaled to have standard deviation 1 and mean 0. For the first component, \\[ z_1 = \\phi_{11}x_1 + \\phi_{21}x_2 +... + \\phi_{p1}x_p\\\\ \\] we need to find the component loading vector \\((\\phi_{11}, \\phi_{21},... \\phi_{p1})\\) that has the largest variance. We want to constrain \\(\\sum_{j1}^{p}{\\phi^2_{j1}=1}\\) otherwise the variance would increase just by increasing the magnitude of the \\(\\phi_{j1}\\)’s. Formally, we want to find the loading vector that \\(\\text{maximises } var(z_1)\\) subject to \\(\\sum_{j1}^{p}{\\phi^2_{j1}=1}\\). Given that \\(z_1\\) has mean zero, \\[ \\begin{aligned} \\text{maximise } var(z_1) &amp;= \\text{maximise } \\frac{1}{n}\\sum_{i=1}^{n}z^2_{ij} \\\\ &amp;= \\text{maximise } \\frac{1}{n}\\sum_{i=1}^{n} \\left(\\sum_{j=1}^{p}\\phi_{j1}x_{ij}. \\right)^2 \\end{aligned} \\] Geometrically the \\(z_1\\) is a projection of the vectors \\(x_1,...,x_p\\) into a real line. We want this projection to have the direction that maximises the variance. Consider the figure below representing only 2 variables \\(x_1,x_2\\). The first component, is the line where the projected points have higher dispersion (variance). The points projected on the line in the right, show a higher variance than the one in the left. Once we have found the first component, the next ones are found in a similar fashion but with the additional requirement of orthogonality, i.e., the components need to be independent of each other (orthogonal). The solution for the maximisation problem above is found using eigen decomposition where the eigenvectors are the components of the PCA. PCA can be used to see how variables get “clustered” together into the components and how groups are characterised in terms of the components. More commonly, PCA is used as a pre-processing step to reduce the dimensionality of the data and then the main components are used in a classification or regression algorithm. The following is an external video produced by Dr. Josh Starmer, creator of the channel StatQuest. It gives a simple overview of the geometrical construction of the PCA that might be useful before you do the readings. 1.2 Readings Read the following chapters of An introduction to statistical learning: 6.3 Dimension Reduction Methods - excluding 6.3.2 12.2 Principal Components Analysis 1.3 Practice session Task 1 - Identify the principal components Using the bdiag.csv, let’s run a PCA for several characteristics of cells. … #libraries that we will need set.seed(1974) #fix the random generator seed #read the dataset bdiag.data &lt;- read.csv(&quot;https://www.dropbox.com/s/fvj7774lmyneab6/bdiag.csv?dl=1&quot;, stringsAsFactors = TRUE) bdiag.pca.data &lt;- bdiag.data[,c(&quot;radius_mean&quot;, &quot;texture_mean&quot;, &quot;perimeter_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;, &quot;compactness_mean&quot;, &quot;concavity_mean&quot;, &quot;concave.points_mean&quot;, &quot;symmetry_mean&quot;, &quot;fractal_dimension_mean&quot;)] bdiag.pca &lt;- prcomp(bdiag.pca.data, scale=TRUE) #pca with scaled variables The prcomp() function will include in its results sdev - the standard deviations of the principal components rotation - the matrix of variable loadings for the components x - the scaled matrix of data times the factor loadings (scores as defined in the An introduction to statistical learning book, page 500) We now can see how much variance is explained by the components. We will use some functions of the package factoextra to plot some of the results. The plot representing the variance explained by the components is called the Scree Plot. library(factoextra) ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.3 ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa summary(bdiag.pca) #variance explained ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.3406 1.5870 0.93841 0.7064 0.61036 0.35234 0.28299 ## Proportion of Variance 0.5479 0.2519 0.08806 0.0499 0.03725 0.01241 0.00801 ## Cumulative Proportion 0.5479 0.7997 0.88779 0.9377 0.97495 0.98736 0.99537 ## PC8 PC9 PC10 ## Standard deviation 0.18679 0.10552 0.01680 ## Proportion of Variance 0.00349 0.00111 0.00003 ## Cumulative Proportion 0.99886 0.99997 1.00000 fviz_eig(bdiag.pca) #scree plot The first 2 components explain approximately 80% of the total variation and 3 components, almost 90%. The number of components that one wants to retain is problem specific and it is a trade-off between information and low-dimensionality. It is also useful to look at the contribution of each variable in the different components. #loading vectors par(mfrow=c(1,3)) fviz_pca_var(bdiag.pca, col.var = &quot;steelblue&quot;) # comp 1 vs 2 fviz_pca_var(bdiag.pca, col.var = &quot;steelblue&quot;,axes=c(1,3)) # comp 1 vs 3 fviz_pca_var(bdiag.pca, col.var = &quot;steelblue&quot;,axes=c(2,3)) # comp 2 vs 3 We can see in the plot above that among the 3 components, fractal_dimension_mean contributes mostly to component 2 whereas e.points_mean is contributing to component 1. TRY IT YOURSELF: Get the PCA using the eigen() function that computes eigenvectors and eigenvalues for a matrix. See the solution code #first scale the data bdiag.scaled &lt;- apply(bdiag.pca.data, 2, scale) #apply scale to columns #get the covariance matrix cov.bdiag &lt;- cov(bdiag.scaled) #Get the eigenvalues and eigenvectors #of the covariance matrix ev.bdiag &lt;- eigen(cov.bdiag) #The sqrt of the eigenvalues are the std #deviations of the compontents sqrt(ev.bdiag$values) #equal to bdiag.pca$sdev #And the eigenvectors are the principal components. ev.bdiag$vector #equal to bdiag.pca$rotation (up to the sign) Task 2 - Use PCA in a prediction model We will continue from Task 1 and use some of the principal components as predictors in a logistic model for the variable diagnosis. The figure below shows the separations of the two diagnoses groups B and M in terms of the the principal components. groups &lt;- bdiag.data$diagnosis fviz_pca_ind(bdiag.pca, col.ind = groups, # color by groups palette = c(&quot;#00AFBB&quot;, &quot;#FC4E07&quot;, &quot;blue&quot;, &quot;red&quot;), addEllipses = TRUE, # Concentration ellipses ellipse.type = &quot;confidence&quot;, legend.title = &quot;Groups&quot;, repel = FALSE, label=&quot;none&quot;, axes=c(1,2) ) The package caret includes PCA as a pre-processing option. We will use 3 principal components to predict diagnosis library(caret) ## Loading required package: lattice ## Registered S3 method overwritten by &#39;future&#39;: ## method from ## all.equal.connection parallelly trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, preProcOptions =list(pcaComp = 3), #or list(thresh = 0.99), classProbs = TRUE, summaryFunction = twoClassSummary) bdiag.glm &lt;- train(diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean, data = bdiag.data, method = &quot;glm&quot;, family=binomial, trControl = trctrl, preProcess=c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;), #uses PCA metric=&quot;ROC&quot;) bdiag.glm ## Generalized Linear Model ## ## 569 samples ## 10 predictor ## 2 classes: &#39;B&#39;, &#39;M&#39; ## ## Pre-processing: centered (10), scaled (10), principal component ## signal extraction (10) ## Resampling: Cross-Validated (10 fold, repeated 1 times) ## Summary of sample sizes: 512, 512, 512, 512, 512, 513, ... ## Resampling results: ## ## ROC Sens Spec ## 0.9834474 0.9605556 0.8863636 The prediction accuracy is excellent with three components. We can get the coefficients for these components but they don’t have an obvious interpretation. summary(bdiag.glm) ## ## Call: ## NULL ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5911 0.2022 -2.923 0.00347 ** ## PC1 -2.7029 0.2882 -9.379 &lt; 2e-16 *** ## PC2 0.8271 0.1509 5.481 4.22e-08 *** ## PC3 0.7430 0.2144 3.465 0.00053 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 751.44 on 568 degrees of freedom ## Residual deviance: 172.10 on 565 degrees of freedom ## AIC: 180.1 ## ## Number of Fisher Scoring iterations: 8 And the principal components matrix: bdiag.glm$preProcess$rotation ## PC1 PC2 PC3 ## radius_mean -0.36393793 0.313929073 -0.12442759 ## texture_mean -0.15445113 0.147180909 0.95105659 ## perimeter_mean -0.37604434 0.284657885 -0.11408360 ## area_mean -0.36408585 0.304841714 -0.12337786 ## smoothness_mean -0.23248053 -0.401962324 -0.16653247 ## compactness_mean -0.36444206 -0.266013147 0.05827786 ## concavity_mean -0.39574849 -0.104285968 0.04114649 ## concave.points_mean -0.41803840 -0.007183605 -0.06855383 ## symmetry_mean -0.21523797 -0.368300910 0.03672364 ## fractal_dimension_mean -0.07183744 -0.571767700 0.11358395 TRY IT YOURSELF: Fit a random forest to predict diagnosis using 7 principal components and cross-validating the number of variables used in each split. See the solution code trctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, preProcOptions =list(pcaComp = 7), classProbs = TRUE, summaryFunction = twoClassSummary) bdiag.rf &lt;- train(diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean + symmetry_mean + fractal_dimension_mean, data = bdiag.data, method = &quot;rf&quot;, family=binomial, trControl = trctrl, preProcess=c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;), #uses PCA ntree = 100, tuneGrid = expand.grid(mtry = c(1:7)), metric=&quot;ROC&quot;) bdiag.rf Task 3 - Use PCA to compress an image This is not a typical Biostats problem, but it illustrates the idea behind PCA. Let’s consider an original image roo.jpg. For simplicity we will only use the green channel information (the original image has 3 channels: red, green and blue). This will correspond to a black and white image. library(jpeg) library(graphics) #the jpeg has 3 channels: red,green, blue #for simplicity of the example, I am only #reading the green channlel roo &lt;- readJPEG(&quot;roo.jpg&quot;)[,,2] #You can have a look at the image plot(1:2, type=&#39;n&#39;, axes=F, ann=F) rasterImage(roo, 1, 2, 2, 1) An image is a matrix of pixels with values that represent the intensity of the the pixel, where 0=white and 1=black. In this case, the image is 800x534 pixels as you can see by the dimension of the matrix. dim(roo) ## [1] 534 800 As we would expect, there are several places in the photo where columns of pixels are strongly correlated with adjacent columns: library(gplots) ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess #color for the heatmap col.correlation &lt;- colorRampPalette(c(&quot;red&quot;,&quot;yellow&quot;,&quot;darkgreen&quot;), space = &quot;rgb&quot;)(30) heatmap.2(cor(roo), Rowv = F, Colv = F, dendrogram = &quot;none&quot;, trace=&quot;none&quot;, col=col.correlation) Therefore, PCA we should be able to reduce the original dimension (800x534) while keeping a substantial part of the information. Out of the 534 components, the first 10 components produce the following image. Note: despite having 800 “variables” columns, there are only 534 rows and we cannot have more components than rows. In fact, we should only have 533 components, but given that we did not centre the data, we have 534. #All the columns with the intensities are in the same scale so #we do not need to scale them #Also, we do not want to centre the columns otherwise #the values can no longer be interpreted as intensities and #we will not be able to plot the results #PCA roo.pca &lt;- prcomp(roo, center = FALSE) #The intensities given by the first 10 components roo.pca10 &lt;- roo.pca$x[,1:10] %*% t(roo.pca$rotation[,1:10]) #this is just to make sure all the values #remain within 0 and 1 (due to rounding in the #calculation, sometimes the values go slightly higher than 1) roo.pca10[roo.pca10&gt;1] &lt;-1 roo.pca10[roo.pca10&lt;0] &lt;-0 #You can have a look at the image par(mfrow=c(1,2)) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title (&quot;original&quot;) rasterImage(roo, 1, 2, 2, 1) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title(&quot;Image with 10 components from PCA&quot;) rasterImage(roo.pca10, 1, 2, 2, 1) Notice that we are using only 10 of the 534 components. If we had used all the components we would have the original image. Let’s see the image with different number of components # Intensities with 20, 50, 100 and 534 components roo.pca.j &lt;- lapply(c(20, 50, 100, 534), function(j) { jcomp &lt;- roo.pca$x[,1:j] %*% t(roo.pca$rotation[,1:j]) jcomp[jcomp&gt;1] &lt;-1 jcomp[jcomp&lt;0] &lt;-0 return(jcomp) } ) par(mfrow=c(2,2)) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title (&quot;20 components&quot;) rasterImage(roo.pca.j[[1]], 1, 2, 2, 1) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title(&quot;50 components&quot;) rasterImage(roo.pca.j[[2]], 1, 2, 2, 1) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title(&quot;100 components&quot;) rasterImage(roo.pca.j[[3]], 1, 2, 2, 1) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title(&quot;534 components (original image&quot;) rasterImage(roo.pca.j[[4]], 1, 2, 2, 1) TRY IT YOURSELF: Produce the image above with 100 components but using the colored photo. See the solution code roo &lt;- readJPEG(&quot;roo.jpg&quot;) # roo is now a list with three elements #corresponding to the channels RBG #we will do PCA in each element roo.rbg.pca&lt;- apply(roo, 3, prcomp, center = FALSE) #Computes the intensities using 100 components roo.pca2 &lt;- lapply(roo.rbg.pca, function(channel.pca) { jcomp &lt;- channel.pca$x[,1:100] %*% t(channel.pca$rotation[,1:100]) jcomp[jcomp&gt;1] &lt;-1 jcomp[jcomp&lt;0] &lt;-0 return(jcomp)}) #Transforms the above list into an array roo.pca2&lt;-array(as.numeric(unlist(roo.pca2)), dim=c(534, 800, 3)) #You can have a look at the image par(mfrow=c(1,2)) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title (&quot;original&quot;) rasterImage(roo, 1, 2, 2, 1) plot(1:2, type=&#39;n&#39;, axes=F, ann=F) title (&quot;100 components&quot;) rasterImage(roo.pca2, 1, 2, 2, 1) 1.4 Exercises Solve the following exercises: The dataset fat is available in the library(faraway). The dataset contains several physical measurements. Using the variables age, weight, height, adipos, free, neck, chest, abdom, hip, thigh, knee, ankle, biceps, forearm and wrist How many components explain at least 95% of the variance? Identify 2 variable that seem to have a stronger contribution to the 2nd principal component. Compare the adjusted-\\(r^2\\) for a linear model for brozek using all the predictors and a linear model only using 2 principal components to predict brozek. "],["k-means-clustering.html", "2 K-means clustering 2.1 Introduction 2.2 Readings 2.3 Practice session 2.4 Exercises", " 2 K-means clustering 2.1 Introduction K-means clustering is a popular unsupervised learning method based on a simple and intuitive approach that clusters similar observations into groups. We start by choosing the number of clusters \\(K\\) and the objective is to assign every observation to one, and just one, of the clusters. The clusters are chosen so that the within-cluster variation is minimised, i.e., the data points in each cluster should be close together. The figure below shows an example of 3 clusters based on two variables. The way the clusters are defined is based on an iterative process. Once we chose the number of clusters: We start by assigning each data point to one of the clusters, randomly. We then compute the centroid of each cluster. Next, we compute the distance (usually the Euclidean distance) of each data point to the centroids. The data points are re-assigned to the corresponding cluster of the closest centroid. The centroids are recomputed We repeat the process until convergence One important question that immediately arises is how many clusters should we consider? Unfortunately, there is not a definitive answer. In the practice session we will show some graphical methods that can be used as an indication of the number of clusters suggested by the data, but as you will see, different methods can suggest different number of clusters. 2.2 Readings Read the following chapters of An introduction to statistical learning: 12.4.1 K-Means Clustering Analysis 2.3 Practice session Task 1 - Identify k clusters Using the bdiag.csv, let’s use 2 of the variables that characterise the cell nuclei - radius_mean and texture_mean - to identify 3 data clusters We will use the function kmeans() with the option centers=3 indicating that we want 3 clusters. #read the dataset bdiag.data &lt;- read.csv(&quot;https://www.dropbox.com/s/vp44yozebx5xgok/bdiag.csv?dl=1&quot;, stringsAsFactors = TRUE) #select a subset of the variables bdiag.2vars &lt;- bdiag.data[,c(&quot;radius_mean&quot;, &quot;texture_mean&quot;)] #let&#39;s compute the 3 clusters km &lt;- kmeans(bdiag.2vars, centers = 3) km ## K-means clustering with 3 clusters of sizes 123, 291, 155 ## ## Cluster means: ## radius_mean texture_mean ## 1 19.55667 21.85732 ## 2 12.43088 16.11027 ## 3 13.00369 23.22110 ## ## Clustering vector: ## [1] 2 1 1 3 1 2 1 3 3 3 3 2 1 3 3 3 3 1 1 2 2 2 2 1 1 2 3 1 3 2 1 2 1 1 2 1 3 ## [38] 2 3 3 3 3 1 3 3 1 2 2 2 3 3 2 2 1 3 2 1 3 2 2 2 3 3 2 3 3 3 2 2 2 1 2 1 2 ## [75] 2 1 2 2 1 2 3 2 1 1 2 1 3 1 3 2 3 3 2 2 3 1 2 3 2 3 3 2 3 2 2 2 2 2 1 3 2 ## [112] 3 3 3 2 3 2 2 3 1 2 1 1 2 2 2 3 1 2 1 2 3 1 2 1 3 2 2 2 2 2 2 2 2 2 2 2 2 ## [149] 2 2 3 3 2 2 2 2 1 1 2 2 3 1 1 3 1 3 2 1 1 2 2 3 2 2 2 2 2 1 3 2 1 1 3 2 3 ## [186] 2 1 2 2 2 3 3 2 3 3 2 3 1 1 3 2 1 1 3 2 2 2 1 3 2 1 2 1 1 3 2 2 2 1 1 2 2 ## [223] 2 3 2 2 2 2 3 3 1 3 3 1 2 3 1 1 3 3 2 2 2 3 1 3 2 2 3 2 1 2 1 2 1 2 1 2 3 ## [260] 3 1 1 1 2 1 1 2 3 2 3 2 2 1 2 1 2 2 1 2 2 1 2 1 1 2 2 3 2 3 2 3 2 2 2 2 2 ## [297] 2 2 2 3 1 3 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 1 2 2 2 2 1 1 2 2 3 ## [334] 2 2 1 2 1 2 1 2 2 2 1 2 2 2 2 2 2 2 2 1 3 2 2 2 2 2 2 2 3 2 2 2 1 1 2 1 1 ## [371] 3 2 1 1 2 2 3 3 2 2 2 2 3 2 2 3 2 2 2 1 2 2 3 1 2 2 2 2 2 2 1 2 2 2 2 2 2 ## [408] 3 1 2 2 2 3 3 3 3 3 3 2 3 2 2 2 2 2 3 2 3 2 2 3 2 1 1 2 3 2 2 3 2 2 1 2 2 ## [445] 1 3 1 2 2 1 3 1 3 2 2 3 3 3 3 3 3 1 3 2 2 3 3 2 1 2 2 3 2 3 2 2 3 2 2 1 2 ## [482] 2 2 2 2 2 2 1 2 1 3 2 1 2 3 3 2 2 1 1 2 3 2 1 2 2 3 2 2 3 2 2 3 2 2 2 1 1 ## [519] 2 2 2 1 3 2 2 2 2 2 2 2 2 3 2 1 2 1 3 3 3 3 2 3 3 3 3 3 2 2 2 3 3 3 3 3 3 ## [556] 3 2 3 3 3 3 3 3 1 1 1 3 1 3 ## ## Within cluster sum of squares by cluster: ## [1] 2001.931 2463.826 2288.434 ## (between_SS / total_SS = 61.5 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; The component km$cluster has the final clusters assignment. We will use the package factoextra that has some plot functions (based on ggplot) that are useful. The fviz_cluster() plots the results of the clusters in a scatter plot formed by the two variables. If the clustering is based on more than 2 variables, this function will run a principal components analysis and plot the first 2 principal components. library(factoextra) fviz_cluster(km, data = bdiag.2vars, label=NA)+theme_bw() TRY IT YOURSELF: Get 2 clusters with k-mean clustering based on the variables age, weight, height, adipos, free, neck, chest, abdom, hip, thigh, knee, ankle, biceps, forearm and wrist . See the solution code #select a subset of the variables bdiag.10vars &lt;- bdiag.data[,c(&quot;radius_mean&quot;, &quot;texture_mean&quot;, &quot;perimeter_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;, &quot;compactness_mean&quot;, &quot;concavity_mean&quot;, &quot;concave.points_mean&quot;, &quot;symmetry_mean&quot;, &quot;fractal_dimension_mean&quot;)] k2 &lt;- kmeans(bdiag.10vars, centers = 2) fviz_cluster(k2, data = bdiag.10vars, label=NA)+theme_bw() Task 2 - Choosing the number of clusters Lets consider the same example as in Task 1 with two variables.How many clusters should we consider? There are some ad-hoc visual methods that may help you guide selecting the number of clusters. The first method is called the Elbow method and consists in computing the k-means clustering for different values of \\(k\\), e.g, by varying \\(k\\) from 1 to 10 clusters then, for each k, calculate the total within-cluster sum of square (\\(wss\\)) and finally, plot the curve of \\(wss\\) according to the number of clusters \\(k\\). In the plot, the location of a bend (knee) suggests the appropriate number of clusters. The function fviz_nbclust() implements this method fviz_nbclust(bdiag.2vars, kmeans, method = &quot;wss&quot;, k.max = 10) The plot above suggest 2 or 3 clusters. Another method is the Average Silhouette Method. In this method we look at the quality of the clustering by measuring how well each data point lies within its cluster. If the average silhouette width is high, this suggests a good clustering. We can then compute the average silhouette width for different values of \\(k\\) and select the number of clusters with higher average silhouette width. The same function as above also implements this method. fviz_nbclust(bdiag.2vars, kmeans, method = &quot;silhouette&quot;, k.max = 10) This method suggests 2 clusters. The final method is the Gap Statistic Method. This method compares the total intracluster variation for different number of cluster \\(k\\) with their expected values under a data with no clustering (these data generated using Monte Carlo simulations). The higher the gap between the observed and expected, the better the clustering. More details about this method is available in R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001) fviz_nbclust(bdiag.2vars, kmeans, method = &quot;gap&quot;, nboot=200, k.max = 10) In this case, the method suggests 1 single cluster. Depending on the method used, we could have selected between 1 to 3 clusters. 2.4 Exercises Solve the following exercises: The dataset fat is available in the library(faraway). The dataset contains several physical measurements. Using the variables age, weight, height, adipos, free, neck, chest, abdom, hip, thigh, knee, ankle, biceps, forearm and wrist Plot 3 clusters produce by k-mean in the scatter plot formed by the two principal components of the data? Use different methods to investigate how many clusters are suggested by the data. "],["hierarchical-clustering.html", "3 Hierarchical Clustering 3.1 Introduction 3.2 Readings 3.3 Practice session 3.4 Exercises", " 3 Hierarchical Clustering 3.1 Introduction Hierarchical clustering is an alternative approach to k-means clustering,which does not require a pre-specification of the number of clusters. The idea of hierarchical clustering is to treat every observation as its own cluster. Then, at each step, we merge the two clusters that are more similar until all observations are clustered together. This can be represented in a tree shaped image called a dendrogram. The height of the branches indicate how different the clusters are. The distance between the groups is usually referred to as linkage. There are 4 types of linkage: Complete linkage: It computes all pairwise dissimilarities between the data points in cluster A and cluster B. The maximum value of these dissimilarities is the distance between the two clusters. Single linkage: Similar to complete linkage but it takes the smallest (minimum) dissimilarity as distance between the two clusters. Average linkage: It computes all pairwise dissimilarities between the data points in cluster A and cluster B and considers the average of these dissimilarities as the distance between the two clusters. Centroid linkage clustering: It computes the dissimilarity between the centroid for cluster A (a mean vector of length p variables) and the centroid for cluster B. Complete and average linkage are more commonly used methods. In terms of dissimilarity measure, we will use the Euclidean distance but there are other options. 3.2 Readings Read the following chapters of An introduction to statistical learning: 12.4.2 Hierarchical Clustering 12.4.3 Practical Issues in Clustering 3.3 Practice session Task 1 - Identify clusters Using the bdiag.csv, let’s use 2 of the variables that characterise the cell nuclei: radius_mean and texture_mean and build a dendrogram We will use the function hclust() to build the dendrogram and the function dist that computes the distances between observations: #read the dataset bdiag.data &lt;- read.csv(&quot;https://www.dropbox.com/s/vp44yozebx5xgok/bdiag.csv?dl=1&quot;, stringsAsFactors = TRUE) #select a subset of the variables bdiag.2vars &lt;- bdiag.data[,c(&quot;radius_mean&quot;, &quot;texture_mean&quot;)] #distances between the observations bdiag.dist &lt;- dist(bdiag.2vars, method = &quot;euclidean&quot;) #### what is dist() doing?################################## bdiag.dist[1] #is the distance between obs1 and obs2 ## [1] 7.82742 bdiag.2vars[1:2, ] #obs 1 and 2 ## radius_mean texture_mean ## 1 17.99 10.38 ## 2 20.57 17.77 sqrt((bdiag.2vars[1, 1] - bdiag.2vars[2,1 ])^2 + (bdiag.2vars[1, 2] - bdiag.2vars[2,2 ])^2 ) #Eucl distance ## [1] 7.82742 ############################################################# #Dendrogram using the complete linkage method bdiag.ddgram &lt;- hclust(bdiag.dist, method=&quot;complete&quot;) #Plot the dendrogram #the option hang = -1 will make the #labels appear below 0 plot(bdiag.ddgram, cex=.4, hang = -1) If we cut the tree at the height of 20, we get 3 clusters plot(bdiag.ddgram, cex=.4, hang = -1) abline(a=20, b=0, lty=2) We can draw a rectangle around the 3 clusters plot(bdiag.ddgram, cex=.4, hang = -1) rect.hclust(bdiag.ddgram, k = 3, border = 2:5) And obtain the cluster for each observation group3 &lt;- cutree(bdiag.ddgram, k = 2) table(group3 ) ## group3 ## 1 2 ## 498 71 #We can also visualise the clusters fviz_cluster(list(data = bdiag.2vars, cluster = group3 )) TRY IT YOURSELF: Get 2 clusters with hierachical clustering using the variables age, weight, height, adipos, free, neck, chest, abdom, hip, thigh, knee, ankle, biceps, forearm and wrist. and compare the clustering result with the observed diagnosis See the solution code #select a subset of the variables bdiag.10vars &lt;- bdiag.data[,c(&quot;radius_mean&quot;, &quot;texture_mean&quot;, &quot;perimeter_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;, &quot;compactness_mean&quot;, &quot;concavity_mean&quot;, &quot;concave.points_mean&quot;, &quot;symmetry_mean&quot;, &quot;fractal_dimension_mean&quot;)] #distances between the observations bdiag.dist10 &lt;- dist(bdiag.10vars, method = &quot;euclidean&quot;) #Dendrogram using the complete linkage method bdiag.ddgram10 &lt;- hclust(bdiag.dist10, method=&quot;complete&quot;) plot(bdiag.ddgram, cex=.4, hang = -1) bdiag.2vars$cluster &lt;- cutree(bdiag.ddgram10, k = 2) table(bdiag.2vars$cluster, bdiag.data$diagnosis) How does the clustering changes with different linkage methods? See the solution code #select a subset of the variables bdiag.10vars &lt;- bdiag.data[,c(&quot;radius_mean&quot;, &quot;texture_mean&quot;, &quot;perimeter_mean&quot;, &quot;area_mean&quot;, &quot;smoothness_mean&quot;, &quot;compactness_mean&quot;, &quot;concavity_mean&quot;, &quot;concave.points_mean&quot;, &quot;symmetry_mean&quot;, &quot;fractal_dimension_mean&quot;)] #distances between the observations bdiag.dist10 &lt;- dist(bdiag.10vars, method = &quot;euclidean&quot;) #Dendrogram using the complete linkage method bdiag.ddgram10.comp &lt;- hclust(bdiag.dist10, method=&quot;complete&quot;) bdiag.ddgram10.sing &lt;- hclust(bdiag.dist10, method=&quot;single&quot;) bdiag.ddgram10.aver &lt;- hclust(bdiag.dist10, method=&quot;average&quot;) bdiag.ddgram10.cent &lt;- hclust(bdiag.dist10, method=&quot;centroid&quot;) bdiag.2vars$cluster.comp &lt;- cutree(bdiag.ddgram10.comp, k = 2) bdiag.2vars$cluster.sing &lt;- cutree(bdiag.ddgram10.sing, k = 2) bdiag.2vars$cluster.aver &lt;- cutree(bdiag.ddgram10.aver, k = 2) bdiag.2vars$cluster.cent &lt;- cutree(bdiag.ddgram10.cent, k = 2) table(bdiag.2vars$cluster.comp, bdiag.2vars$cluster.sing) table(bdiag.2vars$cluster.comp, bdiag.2vars$cluster.aver) table(bdiag.2vars$cluster.comp, bdiag.2vars$cluster.cent) 3.4 Exercises Solve the following exercises: The dataset fat is available in the library(faraway). The dataset contains several physical measurements. Using the variables age, weight, height, adipos, free, neck, chest, abdom, hip, thigh, knee, ankle, biceps, forearm and wrist Plot 3 clusters produce by hierarchical cluster based on the two principal components of the data? Compare the result above with the clusters obtained using all the variables. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
